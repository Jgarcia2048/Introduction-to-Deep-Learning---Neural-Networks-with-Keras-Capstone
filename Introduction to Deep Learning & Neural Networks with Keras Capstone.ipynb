{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "A. Build a baseline model (5 marks)\n",
    "\n",
    "Use the Keras library to build a neural network with the following:\n",
    "\n",
    "- One hidden layer of 10 nodes, and a ReLU activation function\n",
    "\n",
    "- Use the adam optimizer and the mean squared error  as the loss function.\n",
    "\n",
    "1. Randomly split the data into a training and test sets by holding 30% of the data for testing. You can use the train_test_splithelper function from Scikit-learn.\n",
    "\n",
    "2. Train the model on the training data using 50 epochs.\n",
    "\n",
    "3. Evaluate the model on the test data and compute the mean squared error between the predicted concrete strength and the actual concrete strength. You can use the mean_squared_error function from Scikit-learn.\n",
    "\n",
    "4. Repeat steps 1 - 3, 50 times, i.e., create a list of 50 mean squared errors.\n",
    "\n",
    "5. Report the mean and the standard deviation of the mean squared errors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Read in data and place into pandas DF where X and Target are split\n",
    "\n",
    "df = pd.read_csv('concrete_data.csv')\n",
    "X = df.iloc[:,0:8]\n",
    "target = df['Strength']\n",
    "\n",
    "#Converting Dataframes into Numpy Arrays\n",
    "# X = X.to_numpy() \n",
    "# target = target.to_numpy()\n",
    "\n",
    "class LossHistory(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "        self.val_losses = []\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        self.val_losses.append(logs.get('val_loss'))\n",
    "        \n",
    "history = LossHistory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : Train-MSE:  315.5904 , Test-MSE 335.6251\n",
      "2 : Train-MSE:  105.9692 , Test-MSE 110.7327\n",
      "3 : Train-MSE:  397.1444 , Test-MSE 432.9847\n",
      "4 : Train-MSE:  137.4414 , Test-MSE 121.9612\n",
      "5 : Train-MSE:  311.2614 , Test-MSE 401.1513\n",
      "6 : Train-MSE:  122.3309 , Test-MSE 130.8706\n",
      "7 : Train-MSE:  1608.9534 , Test-MSE 1765.3898\n",
      "8 : Train-MSE:  376.1057 , Test-MSE 372.2361\n",
      "WARNING:tensorflow:Callback method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0000s vs `on_train_batch_begin` time: 0.0002s). Check your callbacks.\n",
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0000s vs `on_train_batch_end` time: 0.0003s). Check your callbacks.\n",
      "9 : Train-MSE:  500.7932 , Test-MSE 629.7396\n",
      "10 : Train-MSE:  315.5661 , Test-MSE 394.4383\n",
      "11 : Train-MSE:  135.6141 , Test-MSE 113.2498\n",
      "12 : Train-MSE:  201.278 , Test-MSE 204.7938\n",
      "13 : Train-MSE:  130.6532 , Test-MSE 121.2504\n",
      "14 : Train-MSE:  107.0411 , Test-MSE 112.6479\n",
      "15 : Train-MSE:  456.2754 , Test-MSE 388.3581\n",
      "16 : Train-MSE:  130.2013 , Test-MSE 126.4174\n",
      "17 : Train-MSE:  891.4001 , Test-MSE 826.9845\n",
      "18 : Train-MSE:  135.6812 , Test-MSE 122.9862\n",
      "19 : Train-MSE:  1836.9768 , Test-MSE 1792.2713\n",
      "20 : Train-MSE:  268.1878 , Test-MSE 301.8776\n",
      "21 : Train-MSE:  516.1958 , Test-MSE 530.5747\n",
      "22 : Train-MSE:  219.5401 , Test-MSE 248.9079\n",
      "23 : Train-MSE:  105.4691 , Test-MSE 106.8771\n",
      "24 : Train-MSE:  113.4441 , Test-MSE 103.7491\n",
      "25 : Train-MSE:  149.8424 , Test-MSE 163.3619\n",
      "26 : Train-MSE:  126.6544 , Test-MSE 132.0028\n",
      "27 : Train-MSE:  123.4866 , Test-MSE 118.1788\n",
      "28 : Train-MSE:  600.0286 , Test-MSE 638.7384\n",
      "29 : Train-MSE:  206.8865 , Test-MSE 185.894\n",
      "30 : Train-MSE:  207.176 , Test-MSE 203.4399\n",
      "31 : Train-MSE:  284.875 , Test-MSE 319.0387\n",
      "32 : Train-MSE:  145.4962 , Test-MSE 133.3931\n",
      "33 : Train-MSE:  353.4928 , Test-MSE 345.0023\n",
      "34 : Train-MSE:  127.5556 , Test-MSE 119.5518\n",
      "35 : Train-MSE:  799.9608 , Test-MSE 656.7729\n",
      "36 : Train-MSE:  97.7956 , Test-MSE 85.3793\n",
      "37 : Train-MSE:  142.1333 , Test-MSE 142.8408\n",
      "38 : Train-MSE:  220.0186 , Test-MSE 225.7344\n",
      "39 : Train-MSE:  148.2262 , Test-MSE 145.802\n",
      "40 : Train-MSE:  109.189 , Test-MSE 128.0344\n",
      "41 : Train-MSE:  541.2278 , Test-MSE 548.3271\n",
      "42 : Train-MSE:  120.5742 , Test-MSE 130.9868\n",
      "43 : Train-MSE:  678.3126 , Test-MSE 659.7651\n",
      "44 : Train-MSE:  226.8794 , Test-MSE 227.5846\n",
      "45 : Train-MSE:  117.2923 , Test-MSE 99.33\n",
      "46 : Train-MSE:  112.4163 , Test-MSE 124.1412\n",
      "47 : Train-MSE:  234.9159 , Test-MSE 204.3409\n",
      "48 : Train-MSE:  178.0 , Test-MSE 186.4528\n",
      "49 : Train-MSE:  111.7736 , Test-MSE 138.9895\n",
      "50 : Train-MSE:  173.651 , Test-MSE 167.6968\n",
      "\n",
      "MSE Mean: 320.5371052604069\n",
      "MSE Standard Deviation: 348.9735397888446\n"
     ]
    }
   ],
   "source": [
    "#Build Model\n",
    "def Intro_NN():\n",
    "    mod = Sequential()\n",
    "    cols = X.shape[1]\n",
    "\n",
    "    mod.add(Dense(10, activation='relu', input_shape=(8,)))\n",
    "    mod.add(Dense(1))\n",
    "    mod.compile(optimizer='adam', loss = 'mse')\n",
    "    return mod\n",
    "\n",
    "train_mse = []\n",
    "test_mse = []\n",
    "\n",
    "for i in range(50):\n",
    "    #Splitting dataset\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, target, test_size=0.3)#random_state=37\n",
    "    temp_model = Intro_NN()\n",
    "    temp_model.fit(x_train, y_train, epochs=50, verbose=0, callbacks=[history])\n",
    "    y_pred = temp_model.predict(x_test)\n",
    "    train_mse.append(history.losses[-1])\n",
    "    test_mse.append(mean_squared_error(y_test, y_pred))\n",
    "    print(str(i+1),': Train-MSE: ', round(train_mse[i],4),',',\n",
    "          'Test-MSE', round(test_mse[i],4))\n",
    "    \n",
    "fmean = np.mean(test_mse)\n",
    "print()\n",
    "print(\"MSE Mean: \" + str(fmean))\n",
    "print(\"MSE Standard Deviation: \" + str(np.std(test_mse)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "B. Normalize the data (5 marks)\n",
    "\n",
    "Repeat Part A but use a normalized version of the data. Recall that one way to normalize the data is by subtracting the mean from the individual predictors and dividing by the standard deviation.\n",
    "\n",
    "How does the mean of the mean squared errors compare to that from Step A?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : Train-MSE:  394.1279 , Test-MSE 360.0478\n",
      "2 : Train-MSE:  198.0533 , Test-MSE 190.5256\n",
      "3 : Train-MSE:  662.1433 , Test-MSE 557.5319\n",
      "4 : Train-MSE:  91.137 , Test-MSE 89.9155\n",
      "5 : Train-MSE:  133.693 , Test-MSE 151.6016\n",
      "6 : Train-MSE:  191.9302 , Test-MSE 198.3424\n",
      "7 : Train-MSE:  198.2712 , Test-MSE 192.4194\n",
      "8 : Train-MSE:  418.648 , Test-MSE 458.6033\n",
      "9 : Train-MSE:  407.5598 , Test-MSE 405.1833\n",
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0002s vs `on_train_batch_end` time: 0.0003s). Check your callbacks.\n",
      "10 : Train-MSE:  725.8218 , Test-MSE 710.2741\n",
      "11 : Train-MSE:  136.4014 , Test-MSE 140.7332\n",
      "12 : Train-MSE:  112.3409 , Test-MSE 125.8748\n",
      "13 : Train-MSE:  237.6595 , Test-MSE 248.5358\n",
      "14 : Train-MSE:  84.1207 , Test-MSE 85.308\n",
      "15 : Train-MSE:  115.9922 , Test-MSE 117.9072\n",
      "16 : Train-MSE:  1630.731 , Test-MSE 1552.3898\n",
      "17 : Train-MSE:  564.5106 , Test-MSE 603.2239\n",
      "18 : Train-MSE:  111.9441 , Test-MSE 116.5699\n",
      "19 : Train-MSE:  274.8119 , Test-MSE 236.4078\n",
      "20 : Train-MSE:  785.8748 , Test-MSE 825.8094\n",
      "21 : Train-MSE:  138.9129 , Test-MSE 143.4131\n",
      "22 : Train-MSE:  694.9879 , Test-MSE 748.5804\n",
      "23 : Train-MSE:  113.0646 , Test-MSE 110.6002\n",
      "24 : Train-MSE:  103.5042 , Test-MSE 114.3201\n",
      "25 : Train-MSE:  153.5024 , Test-MSE 150.5664\n",
      "26 : Train-MSE:  121.4821 , Test-MSE 125.6863\n",
      "27 : Train-MSE:  276.0316 , Test-MSE 262.0319\n",
      "28 : Train-MSE:  259.294 , Test-MSE 249.3605\n",
      "29 : Train-MSE:  950.0013 , Test-MSE 1033.3379\n",
      "30 : Train-MSE:  144.1 , Test-MSE 143.021\n",
      "31 : Train-MSE:  177.3125 , Test-MSE 220.0117\n",
      "32 : Train-MSE:  84.6513 , Test-MSE 75.6959\n",
      "33 : Train-MSE:  138.0212 , Test-MSE 148.1793\n",
      "34 : Train-MSE:  91.3111 , Test-MSE 99.704\n",
      "35 : Train-MSE:  106.2986 , Test-MSE 119.6151\n",
      "36 : Train-MSE:  168.1774 , Test-MSE 181.2704\n",
      "37 : Train-MSE:  403.3376 , Test-MSE 409.3798\n",
      "38 : Train-MSE:  123.0449 , Test-MSE 123.3281\n",
      "39 : Train-MSE:  1887.7657 , Test-MSE 1908.5503\n",
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0002s vs `on_train_batch_end` time: 0.0003s). Check your callbacks.\n",
      "40 : Train-MSE:  261.0265 , Test-MSE 261.3257\n",
      "41 : Train-MSE:  256.2369 , Test-MSE 240.5771\n",
      "42 : Train-MSE:  91.8393 , Test-MSE 104.0319\n",
      "43 : Train-MSE:  183.6142 , Test-MSE 181.671\n",
      "44 : Train-MSE:  149.3131 , Test-MSE 150.8444\n",
      "45 : Train-MSE:  494.7141 , Test-MSE 440.7913\n",
      "46 : Train-MSE:  110.8885 , Test-MSE 119.6234\n",
      "47 : Train-MSE:  124.0396 , Test-MSE 114.8476\n",
      "48 : Train-MSE:  1125.2017 , Test-MSE 935.1733\n",
      "49 : Train-MSE:  116.8239 , Test-MSE 110.0784\n",
      "50 : Train-MSE:  152.9656 , Test-MSE 137.5864\n",
      "\n",
      "MSE Mean: 330.6081474125115\n",
      "The mean difference is: -10.07104215210461\n"
     ]
    }
   ],
   "source": [
    "#Normalizing data to scale it from 0 to 1\n",
    "normx = (X - X.mean())/(X.std())\n",
    "msetest = []\n",
    "msetrain = []\n",
    "\n",
    "train_mse = []\n",
    "test_mse = []\n",
    "\n",
    "for i in range(50):\n",
    "    #Splitting dataset\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, target, test_size=0.3)#random_state=37\n",
    "    temp_model = Intro_NN()\n",
    "    temp_model.fit(x_train, y_train, epochs=50, verbose=0, callbacks=[history])\n",
    "    y_pred = temp_model.predict(x_test)\n",
    "    train_mse.append(history.losses[-1])\n",
    "    test_mse.append(mean_squared_error(y_test, y_pred))\n",
    "    print(str(i+1),': Train-MSE: ', round(train_mse[i],4),',',\n",
    "          'Test-MSE', round(test_mse[i],4))\n",
    "\n",
    "tmean = np.mean(test_mse)\n",
    "diff = fmean - tmean\n",
    "print()\n",
    "print(\"MSE Mean: \" + str(tmean))\n",
    "print(\"The mean difference is: \" + str(diff))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "C. Increase the number of epochs (5 marks)\n",
    "\n",
    "Repeat Part B but use 100 epochs this time for training.\n",
    "\n",
    "How does the mean of the mean squared errors compare to that from Step B?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : Train-MSE:  212.9275 , Test-MSE 250.9217\n",
      "2 : Train-MSE:  113.1351 , Test-MSE 118.4904\n",
      "3 : Train-MSE:  147.4171 , Test-MSE 166.3145\n",
      "4 : Train-MSE:  148.7467 , Test-MSE 156.986\n",
      "5 : Train-MSE:  114.1381 , Test-MSE 124.3532\n",
      "6 : Train-MSE:  291.4152 , Test-MSE 312.2502\n",
      "7 : Train-MSE:  110.8105 , Test-MSE 115.0262\n",
      "8 : Train-MSE:  129.9355 , Test-MSE 123.8516\n",
      "9 : Train-MSE:  112.5376 , Test-MSE 103.8225\n",
      "10 : Train-MSE:  106.2691 , Test-MSE 123.1566\n",
      "11 : Train-MSE:  113.9643 , Test-MSE 115.2914\n",
      "12 : Train-MSE:  112.7902 , Test-MSE 104.8758\n",
      "13 : Train-MSE:  128.2462 , Test-MSE 145.0477\n",
      "14 : Train-MSE:  110.1428 , Test-MSE 99.5903\n",
      "15 : Train-MSE:  120.6426 , Test-MSE 140.516\n",
      "16 : Train-MSE:  248.9829 , Test-MSE 254.4109\n",
      "17 : Train-MSE:  99.9253 , Test-MSE 112.0227\n",
      "18 : Train-MSE:  94.1727 , Test-MSE 91.6616\n",
      "19 : Train-MSE:  122.0304 , Test-MSE 119.3265\n",
      "20 : Train-MSE:  236.9523 , Test-MSE 268.5547\n",
      "21 : Train-MSE:  121.1541 , Test-MSE 115.486\n",
      "22 : Train-MSE:  111.701 , Test-MSE 117.8033\n",
      "23 : Train-MSE:  154.1776 , Test-MSE 139.2684\n",
      "24 : Train-MSE:  69.3402 , Test-MSE 79.6278\n",
      "25 : Train-MSE:  95.9736 , Test-MSE 114.0194\n",
      "26 : Train-MSE:  167.1421 , Test-MSE 146.1548\n",
      "27 : Train-MSE:  149.2359 , Test-MSE 141.0775\n",
      "28 : Train-MSE:  81.3716 , Test-MSE 84.3969\n",
      "29 : Train-MSE:  120.409 , Test-MSE 120.61\n",
      "30 : Train-MSE:  136.1979 , Test-MSE 112.1807\n",
      "31 : Train-MSE:  79.7028 , Test-MSE 82.2482\n",
      "32 : Train-MSE:  70.1532 , Test-MSE 78.2673\n",
      "33 : Train-MSE:  114.8828 , Test-MSE 107.6215\n",
      "34 : Train-MSE:  112.4908 , Test-MSE 117.6246\n",
      "35 : Train-MSE:  161.5467 , Test-MSE 169.9927\n",
      "36 : Train-MSE:  128.658 , Test-MSE 130.8301\n",
      "37 : Train-MSE:  105.3429 , Test-MSE 115.6348\n",
      "38 : Train-MSE:  148.3358 , Test-MSE 155.2386\n",
      "39 : Train-MSE:  128.4517 , Test-MSE 144.1099\n",
      "40 : Train-MSE:  141.4276 , Test-MSE 120.5505\n",
      "41 : Train-MSE:  105.8636 , Test-MSE 117.7643\n",
      "42 : Train-MSE:  219.3763 , Test-MSE 227.0341\n",
      "43 : Train-MSE:  160.9257 , Test-MSE 189.1563\n",
      "44 : Train-MSE:  153.7555 , Test-MSE 164.2946\n",
      "45 : Train-MSE:  116.3911 , Test-MSE 117.6045\n",
      "46 : Train-MSE:  395.8016 , Test-MSE 364.6488\n",
      "47 : Train-MSE:  114.3708 , Test-MSE 117.4005\n",
      "48 : Train-MSE:  134.8387 , Test-MSE 155.6437\n",
      "49 : Train-MSE:  152.1624 , Test-MSE 163.8908\n"
     ]
    }
   ],
   "source": [
    "msetest = []\n",
    "msetrain = []\n",
    "\n",
    "for i in range(50):\n",
    "    #Splitting dataset\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, target, test_size=0.3)#random_state=37\n",
    "    temp_model = Intro_NN()\n",
    "    temp_model.fit(x_train, y_train, epochs=100, verbose=0, callbacks=[history])\n",
    "    y_pred = temp_model.predict(x_test)\n",
    "    msetrain.append(history.losses[-1])\n",
    "    msetest.append(mean_squared_error(y_test, y_pred))\n",
    "    print(str(i+1),': Train-MSE: ', round(msetrain[i],4),',',\n",
    "          'Test-MSE', round(msetest[i],4))\n",
    "\n",
    "print()\n",
    "print(\"MSE Mean: \" + str(np.mean(msetest)))\n",
    "print(\"MSE Standard Deviation: \" + str(np.std(msetest)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "D. Increase the number of hidden layers (5 marks)\n",
    "\n",
    "Repeat part B but use a neural network with the following instead:\n",
    "\n",
    "- Three hidden layers, each of 10 nodes and ReLU activation function.\n",
    "\n",
    "How does the mean of the mean squared errors compare to that from Step B?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%"
    }
   },
   "outputs": [],
   "source": [
    "#Normalizing data to scale it from 0 to 1\n",
    "normx = (X - X.mean())/(X.std())\n",
    "msetest = []\n",
    "msetrain = []\n",
    "\n",
    "def Intro_NN_partD():\n",
    "    mod = Sequential()\n",
    "    mod.add(Dense(10, activation='relu', input_shape=(8,)))\n",
    "    mod.add(Dense(10, activation='relu'))\n",
    "    mod.add(Dense(10, activation='relu'))\n",
    "    mod.add(Dense(1))\n",
    "    mod.compile(optimizer='adam', loss = 'mse')\n",
    "    return mod\n",
    "\n",
    "train_mse = []\n",
    "test_mse = []\n",
    "\n",
    "for i in range(50):\n",
    "    #Splitting dataset\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, target, test_size=0.3)#random_state=37\n",
    "    temp_model = Intro_NN_partD()\n",
    "    temp_model.fit(x_train, y_train, epochs=50, verbose=0, callbacks=[history])\n",
    "    y_pred = temp_model.predict(x_test)\n",
    "    train_mse.append(history.losses[-1])\n",
    "    test_mse.append(mean_squared_error(y_test, y_pred))\n",
    "    print(str(i+1),': Train-MSE: ', round(train_mse[i],4),',',\n",
    "          'Test-MSE', round(test_mse[i],4))\n",
    "\n",
    "tmean2 = np.mean(test_mse)\n",
    "diff = fmean - tmean\n",
    "print()\n",
    "print(\"MSE Mean for Part B: \" + str(tmean))\n",
    "print(\"MSE Mean for Part D: \" + str(tmean2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}